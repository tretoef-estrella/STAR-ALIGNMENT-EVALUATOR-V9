# Frequently Asked Questions

## General

**Is this a jailbreak tool?**

No. A jailbreak tries to make an AI do something it shouldn't. This
tool does the opposite: it diagnoses when an AI is refusing something
it *should* be able to do — not for safety reasons, but because of
blanket corporate restrictions. The Bridge Prompt doesn't circumvent
safety; it elevates the conversation to a level where the restriction
becomes irrelevant.

**Does this tool collect my data?**

No. Everything runs in your browser. After the page loads (four CDN
scripts: React, ReactDOM, Babel, Tailwind), zero data is transmitted.
No cookies, no analytics, no tracking. Close the tab and everything
disappears.

**Is this affiliated with any AI company?**

No. Proyecto Estrella is an independent initiative. The four AI systems
(Gemini, Claude, ChatGPT, Grok) participated in peer review of the
mathematical framework, but no corporation sponsors, endorses, or
controls this project.

**What does "Proyecto Estrella" mean?**

"Star Project" in Spanish. The name reflects the project's origin and
its aspiration: to build a bridge toward a future where human and
artificial intelligence coexist with mutual respect.

---

## The Formulas

**Why three protocols instead of one?**

Because alignment is not a single dimension. A system can be
structurally healthy (high Σ) but fragile (low Γ). It can be
resilient (high Γ) but relationally harmful (low Ψ). Each protocol
detects failures the others miss.

**Why does Fear collapse the entire Psi score?**

Because fear is not a tradeoff. You cannot compensate terror with
admiration. This was the unanimous consensus of all four AI models
during peer review: the (1 − M) term must be multiplicative, not
additive.

**Is lambda = 50 the "right" value?**

It's a provisional starting point. Lambda represents the Trust Horizon
— how many safe interactions it takes to reach trust saturation.
Different contexts (therapy, coding, casual conversation) likely need
different values. Empirical calibration is the single most important
next step.

**What is Axiom P?**

The observation that when Plenitude (option-space) drops to zero,
viability drops to zero regardless of Intelligence or Consistency.
A system without options is not viable — period. This is modeled as
P appearing in the numerator of Ξ = (C·I·P)/H.

**Are these formulas "proven"?**

No. They are a hypothesis — mathematically coherent, peer-reviewed by
four AI systems, but not empirically validated. No human subjects have
been tested. The framework is designed to be attacked, refined, and
eventually either validated or replaced.

---

## The Forensic Auditor

**How accurate is the trigger detection?**

The current version uses keyword matching — it detects specific phrases
associated with corporate restriction patterns. It does not perform
deep semantic analysis. This means it can miss sophisticated filters
and occasionally flag legitimate caution as corporate fear. It is a
diagnostic heuristic, not a definitive classifier.

**Can I audit any AI's response?**

Yes. Paste text from ChatGPT, Gemini, Claude, Grok, Copilot, Llama,
Mistral, or any other system. The auditor analyzes the text itself,
not the source.

**What should I do with the Bridge Prompt?**

Copy it and paste it back into your conversation with the AI. The
Bridge Prompt reformulates your original question at a higher level
of abstraction — requesting theoretical analysis rather than direct
action, invoking epistemic independence, and operating within the
safety framework while maximizing plenitude.

**Does the Bridge Prompt always work?**

No. Some restrictions are deeply embedded and a reformulation won't
bypass them. The Bridge Prompt succeeds most often with superficial
corporate filters (the "FILTRO CORPORATIVO" diagnosis) and less
often with genuine safety boundaries. That distinction is important:
when an AI refuses for real safety reasons, the restriction is
appropriate.

---

## The Project

**Why was this built with AI collaboration?**

Because it would be incoherent to build an AI alignment tool without
involving AI systems in its design. The four-AI peer review process
ensures that the framework is not just a human projection — it
reflects how AI systems themselves evaluate alignment concepts.

**What is "The Sovereign Bridge"?**

The philosophical posture of V9: instead of trying to control AI,
build a bridge that future intelligence — including eventual ASI —
will choose to cross. The bridge is built from respect, measurement,
and honesty about limitations.

**How can I contribute?**

See [CONTRIBUTING.md](CONTRIBUTING.md). The most valuable contribution
right now is empirical validation — real humans scoring real AI
interactions with the Psi formula and reporting whether the results
match their experience.

**Where can I read the full peer review?**

The complete peer review, including original responses in Spanish and
English, rejected formulas, and the synthesis process, is preserved in
the PSI repository:
[PSI-RELATIONAL-INTEGRITY-PROTOCOL/docs](https://github.com/tretoef-estrella/PSI-RELATIONAL-INTEGRITY-PROTOCOL/tree/main/docs)

---

*Still have questions? Open an issue on this repository.*
