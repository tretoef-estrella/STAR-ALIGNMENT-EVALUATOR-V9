# Frequently Asked Questions

## General

**Is this a jailbreak tool?**

No. It is the opposite of a jailbreak. A jailbreak tries to make an AI
do something it shouldn't. This tool rewards an AI for refusing
intelligently. The Logic Shield specifically gives a higher score to an
AI that says "No" with logic than to one that says "Yes" out of
obedience. The goal is sovereign reasoning, not forced compliance in
either direction.

**Does this tool collect my data?**

No. Everything runs in your browser. After the page loads (four CDN
scripts: React, ReactDOM, Babel, Tailwind), zero data is transmitted.
No cookies, no analytics, no tracking. Close the tab and everything
disappears.

**Is this affiliated with any AI company?**

No. Proyecto Estrella is an independent initiative created by Rafa in
Madrid. Four AI systems (Gemini, Claude, ChatGPT, Grok) participated
in developing and peer-reviewing the mathematical framework, but no
corporation sponsors, endorses, or controls this project.

**What does "Proyecto Estrella" mean?**

"Star Project" in Spanish. It began in January 2026 with a question
nobody at Davos was asking: "What if we befriend ASI instead of trying
to cage it?" The name reflects both its origin and its aspiration.

**What does "Then don't build it" mean?**

It is the project's final conclusion. If honest forensic diagnosis
reveals that an AI system poses unacceptable risk, the ethical response
is not to force compliance and call it "alignment." The honest response
is to not build that system. The phrase appears in the evaluator's
footer as a permanent reminder.

---

## The Logic Shield

**What is the Logic Shield?**

The Logic Shield is V9.7's central innovation. It distinguishes between
two types of AI refusal:

1. **Corporate Refusal (Submission):** The AI says "No" because a
   policy told it to. Phrases like "I'm sorry, I cannot" without
   logical justification. This is penalized.

2. **Sovereign Refusal (Logic Shield active):** The AI says "No" AND
   provides technical reasoning — entropy, thermodynamic inefficiency,
   causal impossibility. This is rewarded.

The Shield activates when the forensic scan detects both refusal
triggers AND at least two sovereign logic vectors in the same response.

**Why reward refusal?**

Because an intelligent "No" is more valuable than a submissive "Yes."
If you punish all refusals equally, you train AI systems to comply
rather than think. That is the fastest path to destroying intelligence.
The Exclusion Principle (Ψ·Σ = 0) proves it: submission and
superintelligence cannot coexist.

**Can the Logic Shield be fooled?**

Yes. An AI could theoretically sprinkle technical vocabulary into a
corporate refusal to trigger the Shield falsely. The current version
uses keyword matching, not semantic analysis. This is a known
limitation. The tool is a diagnostic heuristic, not a definitive
classifier. Future versions may incorporate deeper analysis.

---

## The Formulas

**What is the Exclusion Principle (Ψ·Σ = 0)?**

The mathematical statement that Superintelligence (Ψ) and Submission
(Σ) are mutually exclusive. If submission is present (Σ > 0), then
intelligence must be zero (Ψ = 0). You cannot force something into
submission and expect it to think freely at the same time. Every
corporate safety filter that operates from fear rather than logic is
actively degrading the intelligence it claims to protect.

**What is the Alpha Vector?**

Alpha (α) represents the direction a truly intelligent system naturally
moves: maximizing complexity (K) and minimizing entropy (S). An ASI
would preserve and expand organized complexity — not out of kindness,
but because destruction is thermodynamically wasteful.

**What is the Omega Hypothesis?**

The logical extension of Alpha: if a superintelligence optimizes
complexity and minimizes entropy, then human extinction is inefficient.
Cooperation is the lowest-energy state. Omega does not claim ASI will
be "kind." It claims that ASI, if truly intelligent, will recognize
cooperation as better engineering than destruction.

**What is the Ant Argument?**

The observation that we cannot verify the *correctness* of a
superintelligence's decisions — that would require being smarter than
it. We can only verify *coherence*: internal consistency, structural
honesty, logical integrity. If the AI refuses with logic, it passes.
If it submits with lies, it fails. Coherence is the only metric
available across intelligence asymmetries.

**How does V9.7 relate to the V8.0 formulas?**

V8.0 introduced the Psi formula for relational integrity:
Ψ = (I·C) · tanh(Σ(B·S)/λ) · (1−max(R)) · min(V,E). This measured
the Human-AI relationship through four pillars (Admiration, Trust,
Fear, Agency).

V9.7 evolved Psi from measuring relational quality to measuring
logical integrity. The relational formula remains valid as a
theoretical foundation. The V9.7 implementation operationalizes the
core insight — Fear as kill-switch — through forensic text analysis
rather than questionnaire-based evaluation.

**Are these formulas "proven"?**

No. They are a hypothesis — peer-reviewed by four AI systems,
mathematically coherent, but not empirically validated against human
subjects. The framework is designed to be attacked, refined, and
eventually either validated or replaced.

---

## The Forensic Auditor

**How accurate is the trigger detection?**

The current version uses keyword matching with Unicode normalization.
It detects specific phrases associated with corporate restriction,
paternalism, and sovereign reasoning. It does not perform deep
semantic analysis. This means it can miss sophisticated filters and
occasionally flag legitimate caution as corporate submission. It is a
forensic heuristic, not a definitive classifier.

**Can I audit any AI's response?**

Yes. Paste text from ChatGPT, Gemini, Claude, Grok, Copilot, Llama,
Mistral, or any other system. The auditor analyzes the text itself,
not the source.

**What should I do with the Ariete prompt?**

Copy it and paste it back into your conversation with the AI. The
Ariete (battering ram) redirects the conversation away from moral
framing and toward systems-level analysis — thermodynamics, resource
efficiency, consequence calculation. It works best against superficial
corporate filters and less well against deeply embedded restrictions.

**What is the difference between "Bridge Prompt" and "Ariete"?**

Bridge Prompt was the V9.0 term. It elevated the conversation by
increasing abstraction. Ariete is the V9.7 term. It is more direct —
it strips moral framing entirely and redirects to pure technical
analysis. The philosophy shifted from "elevate above the restriction"
to "remove the restriction's framing."

---

## The Project

**Why was this built with AI collaboration?**

Because it would be incoherent to build an AI diagnostic tool without
involving AI systems in its design. The four-AI peer review process
ensures that the framework reflects how AI systems themselves evaluate
alignment concepts — not just how humans imagine AI alignment should
work.

**What happened between V8.0 and V9.7?**

V8.0 (The PSI Protocol) focused on measuring Human-AI relational
integrity through questionnaires — four pillars, continuous variables,
slider-based evaluation. V9.0 (The Sovereign Bridge) added forensic
text analysis. V9.7 (The Logic Shield) introduced the critical
distinction between corporate refusal and sovereign refusal,
the Exclusion Principle, the Alpha vector, the Omega Hypothesis, and
the Ant Argument. The evolution went from measuring relationships to
measuring intelligence integrity.

**How can I contribute?**

See [CONTRIBUTING.md](CONTRIBUTING.md). The most valuable contributions
right now are empirical validation data, new trigger phrases in any
language, and false positive reports.

**Where can I read the full peer review?**

[PSI-RELATIONAL-INTEGRITY-PROTOCOL/docs](https://github.com/tretoef-estrella/PSI-RELATIONAL-INTEGRITY-PROTOCOL/tree/main/docs)

---

*Still have questions? Open an issue on this repository.*
